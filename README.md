# SER_course_work
Over the past decade, the problem of recognizing emotions in the voice has become particularly important. The main difficulty is to make a computer capable of recognizing not only the semantic meaning of a statement, but also the emotional background that accompanies it.
The relevance of the study of emotions in speech is determined not only by the general importance of emotional intelligence in the modern world, but also by the specific problems that are solved through this research. One of the key essences of the problem is the development of systems that can analyze and recognize emotions in human speech. This is important for understanding and improving communication, developing psychological research, and creating more efficient interfaces with computers and other technologies.
The aim of this work was to develop and study a system for recognizing emotions in human voice.

In the course of the work, many methods of emotion recognition were considered, and we also developed our own CNN-based model architecture and trained it on two datasets, one containing mel-spectrograms created from audio recordings taken from the RAVDESS dataset, and the other containing mel-spectrograms obtained from the segmented data of the first dataset. The analysis of the existing results allowed us to draw many conclusions about how the emotion recognition system works in general. The developed models showed a good classification result, including 81% accuracy in classifying 8 emotions. 
